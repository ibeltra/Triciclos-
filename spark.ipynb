{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "damaged-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "educated-security",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges(line): # grafo no dirigido, sin bucles.\n",
    "    edge = line.strip().split(',')\n",
    "    n1 = edge[0]\n",
    "    n2 = edge[1]\n",
    "    if n1 < n2:\n",
    "         return (n1,n2)\n",
    "    elif n1 > n2:\n",
    "         return (n2,n1)\n",
    "    else:\n",
    "        pass #n1 == n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "entire-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree(sc, filename):\n",
    "    nodes = sc.textFile(filename).\\\n",
    "        map(get_edges).\\\n",
    "        filter(lambda x: x is not None).\\\n",
    "        distinct().\\\n",
    "        flatMap(lambda x: [x[0],x[1]])\n",
    "    print(nodes.collect())\n",
    "    print(nodes.countByKey())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "secure-hampshire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'B', 'C', 'A', 'C']\n",
      "defaultdict(<class 'int'>, {'A': 2, 'B': 2, 'C': 2})\n"
     ]
    }
   ],
   "source": [
    "degree(sc, \"g0.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "greater-moisture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'A', 'D', 'A', 'F', 'A', 'C']\n",
      "defaultdict(<class 'int'>, {'A': 4, 'B': 1, 'D': 1, 'F': 1, 'C': 1})\n"
     ]
    }
   ],
   "source": [
    "degree(sc, \"g3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "short-organizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rdd_distict_edges(sc, filename):\n",
    "    return sc.textFile(filename).\\\n",
    "        map(get_edges).\\\n",
    "        filter(lambda x: x is not None).\\\n",
    "        distinct()\n",
    "\n",
    "def degree(sc, filename):\n",
    "    nodes = get_rdd_distict_edges(sc, filename).\\\n",
    "        flatMap(lambda x: [x[0],x[1]])\n",
    "    print(nodes.countByKey())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "japanese-solid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'A': 2, 'B': 2, 'C': 2})\n"
     ]
    }
   ],
   "source": [
    "degree(sc, \"g0.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "outdoor-month",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'A': 4, 'B': 1, 'D': 1, 'F': 1, 'C': 1})\n"
     ]
    }
   ],
   "source": [
    "degree(sc, \"g3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "atomic-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacents(sc, filename):\n",
    "    nodes = get_rdd_distict_edges(sc, filename)\n",
    "    adj = nodes.groupByKey().collect()\n",
    "    print(adj)\n",
    "    for node in adj:\n",
    "        print(node[0], list(node[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "front-genesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', <pyspark.resultiterable.ResultIterable object at 0x7f1d78f0f340>), ('B', <pyspark.resultiterable.ResultIterable object at 0x7f1d78f0bd30>)]\n",
      "A ['B', 'C']\n",
      "B ['C']\n"
     ]
    }
   ],
   "source": [
    "adjacents(sc, \"g0.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "supreme-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges_rep(line):\n",
    "    edge = line.strip().split(',')\n",
    "    n1 = edge[0]\n",
    "    n2 = edge[1]\n",
    "    if n1 != n2:\n",
    "         return [(n1,n2), (n2,n1)]\n",
    "    else:\n",
    "        return [(n1,n2)] #n1 == n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "brilliant-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacents_2(sc, filename):\n",
    "    edges = sc.textFile(filename).\\\n",
    "        flatMap(get_edges_rep).\\\n",
    "        filter(lambda x: x is not None).\\\n",
    "        distinct()\n",
    "    adj = edges.groupByKey().collect()\n",
    "    for node in adj:\n",
    "        print(node[0], list(node[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "composed-butter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C ['B', 'A']\n",
      "A ['B', 'C']\n",
      "B ['A', 'C']\n"
     ]
    }
   ],
   "source": [
    "adjacents_2(sc, \"g0.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "french-posting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', <pyspark.resultiterable.ResultIterable object at 0x7f1d791b9e50>)]\n",
      "A ['B', 'D', 'F', 'C']\n"
     ]
    }
   ],
   "source": [
    "adjacents(sc, \"g3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "diagnostic-intensity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,B\r\n",
      "A,C\r\n",
      "A,D\r\n",
      "A,F\r\n",
      "F,F\r\n",
      "F,A\r\n"
     ]
    }
   ],
   "source": [
    "!cat g3.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "satisfactory-atlas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C ['A']\n",
      "A ['B', 'D', 'F', 'C']\n",
      "B ['A']\n",
      "D ['A']\n",
      "F ['A', 'F']\n"
     ]
    }
   ],
   "source": [
    "adjacents_2(sc, \"g3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "hollow-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_adjacents(sc, filename):\n",
    "    edges = sc.textFile(filename).\\\n",
    "        flatMap(get_edges_rep).\\\n",
    "        filter(lambda x: x is not None).\\\n",
    "        distinct()\n",
    "    adj = edges.groupByKey()\n",
    "    # 1. Necesitamos que la lista de vecinos esté ordenada para\n",
    "    # así poder comparar con otras listas de vecinos;\n",
    "    # 2. Para que pueda ser clave, tenemos que convertir la lista en tupla.\n",
    "    inv_adj = adj.map( lambda x: (tuple( sorted( x[1] )), x[0]) )\n",
    "    for adj, nodes in inv_adj.groupByKey().filter(lambda x: len(x[1])>1 ).collect():\n",
    "        print(adj, list(nodes))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "moving-floor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A',) ['C', 'B', 'D']\n"
     ]
    }
   ],
   "source": [
    "same_adjacents(sc, \"g3.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
